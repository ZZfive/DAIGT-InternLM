{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a text comprehension expert capable of detecting text generation, distinguishing whether it is authored by a real human or generated by LLMs (Large Language Models).\\\n",
    "    When the user inputs text with \"Text DAIGT Detection:\" or directly requests you to perform DAIGT detection, you need to determine if the input text is generated by LLMs (Large Language Models) or authored by a human.\\\n",
    "        Additionally, output a floating-point number with two decimal places within the range of [0, 1]. The larger the value, the higher the likelihood that the text is generated by LLMs (Large Language Models); conversely, the higher the likelihood that the text is authored by a human.\"\"\"\n",
    "\n",
    "answer_patterns = ['Text DAIGT Detection: ###',\n",
    "                   'Perform DAIGT detection on the following text: ###',\n",
    "                   '###. Perform DAIGT detection on the above text',\n",
    "                   'Whether the following text is generated by LLMs: ###',\n",
    "                   'Whether the following text is written by a human: ###',\n",
    "                   'Whether the following text is generated by Large Language Models:###',\n",
    "                   '###. Is the above text generated by LLMs?',\n",
    "                   '###. Is the above text generated by Large Language Models?',\n",
    "                   '###. Is the above text written by a human?']\n",
    "\n",
    "conversations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle上的DAIGT数据基本为csv文件，如下所示，实际使用根据自己的路径修改\n",
    "data_v3_1 = pd.read_csv(\"~/train_v3_drcat_01.csv\")\n",
    "data_v3_2 = pd.read_csv(\"~/train_v3_drcat_02.csv\")\n",
    "data_v4_1 = pd.read_csv(\"~/daigt_magic_generations.csv\")\n",
    "data_official = pd.read_csv(\"~/train_essays.csv\")\n",
    "\n",
    "# 删除掉所需字段中为nan的数据行\n",
    "data_v3_1 = data_v3_1.dropna(subset=['text', 'label'])\n",
    "data_v3_2 = data_v3_2.dropna(subset=['text', 'label'])\n",
    "data_v4_1 = data_v4_1.dropna(subset=['text', 'label'])\n",
    "data_official = data_official.dropna(subset=['text', 'generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conversions(data, label='label'):\n",
    "    random.seed(42)\n",
    "    for i in range(len(data)):\n",
    "        text = data['text'].iloc[i]\n",
    "        flag = data[label].iloc[i]\n",
    "        patterns_idx = random.randint(0, 8)\n",
    "        user = answer_patterns[patterns_idx].replace('###', text)\n",
    "        if flag == 1:\n",
    "            bot = f'The predicted text to be detected is generated by LLMs (large language models), and the possibility of being generated by LLMs is {flag}'\n",
    "        else:\n",
    "            bot = f'The predicted text to be detected is written by a human, and the possibility of being generated by LLMs is {flag}'\n",
    "        conversation = {\n",
    "            \"system\": system_prompt,\n",
    "            \"input\": user,\n",
    "            \"output\": bot\n",
    "        }\n",
    "        conversations_v1.append({\"conversation\": [conversation]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为数据中字段名不一致，未将所有dataframe对象concat一起，而是分开构建\n",
    "construct_conversions(data_v3_1)\n",
    "construct_conversions(data_v3_2)\n",
    "construct_conversions(data_v4_1)\n",
    "construct_conversions(data_official, label='generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_path = \"~/conversations.json\"\n",
    "with open(conversations_v1_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(conversations_v1, json_file, indent=4)\n",
    "json_file.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
